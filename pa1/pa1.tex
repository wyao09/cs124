\documentclass[12pt]{article}

%AMS-TeX packages
\usepackage{amssymb,amsmath,amsthm} 
%geometry (sets margin) and other useful packages
\usepackage[margin=1.25in]{geometry}
\usepackage{graphicx,ctable,booktabs}


%Redefining sections as problems
%
\makeatletter
\newenvironment{problem}{\@startsection
       {section}
       {1}
       {-.2em}
       {-3.5ex plus -1ex minus -.2ex}
       {2.3ex plus .2ex}
       {\pagebreak[3]%forces pagebreak when space is small; use \eject for better results
       \large\bf\noindent{Problem }
       }
       }
       {%\vspace{1ex}\begin{center} \rule{0.3\linewidth}{.3pt}\end{center}}
       \begin{center}\large\bf \ldots\ldots\ldots\end{center}}
\makeatother


%
%Fancy-header package to modify header/page numbering 
%
\usepackage{fancyhdr}
\pagestyle{fancy}
%\addtolength{\headwidth}{\marginparsep} %these change header-rule width
%\addtolength{\headwidth}{\marginparwidth}
\lhead{Problem \thesection}
\chead{} 
\rhead{\thepage} 
\lfoot{\small\scshape CS124} 
\cfoot{} 
\rfoot{\footnotesize PA 1 Writeup} 
\renewcommand{\headrulewidth}{.3pt} 
\renewcommand{\footrulewidth}{.3pt}
\setlength\voffset{-0.25in}
\setlength\textheight{648pt}

%1%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%
%Contents of problem set
%    
\begin{document}

\title{CS124: PA 1 Writeup}
\author{Wen-Yuan Yao & Aidan Daly}
\date{3/1/2012}
\thispagestyle{empty}

\begin{problem}{}
A table or graph listing the average tree size for serveral values of n.

\end{problem}

\begin{problem}{}
A description of your guess for the function $f(n)$.

The growth function for 0 dimensions (random weights) was constant with respect to $n$ - roughly 1.2.

The growth function we determined for dimensions 1 or higher was:
\begin{equation}
f(n) = \frac{2}{3}\frac{n}{n^{1/k}}
\end{equation}
Where $k$ is the dimension of the problem.  The way we came to this function, which agrees very nicely with our empirical data, is as follows:

For any graph of size $n$, the minimum spanning tree will contain $n-1$ edges.  Thus, the total weight of the tree will obviously be proportional to $n$.  We figured that we could predict the weight of the tree by multiplying $n$ by the average weight of an edge we'd expect to be included in the tree.  We reasoned the average weight of such edges by first considering the one-dimensional case.

In the one-dimensional case, we'd expect nodes to be uniformly distributed on a line between 0 and 1.  Thus, we'd expect the average distance between any two adjacent nodes to be $\frac{1}{n}$.  Since the MST in 1 dimension is simply connecting all the adjacent nodes, we'd thus expect the average weight of an edge included in the MST to be $\frac{1}{n}$, which is consistent with our prediction.  Looking at the higher dimensional cases, it is still generally the case that edges added to the MST will be between "adjacent" nodes (i.e., edges between a node and its nearest neighbors).  We still expect points to be uniformly distributed in our unit shape at higher dimensions, and thus distance between a node and its nearest neighbors is still expected to be $\frac{1}{n^k}$.  Thus, assuming that edges are generally added between nodes and their nearest neighbors (which is generally true), the average weight of a node included in the MST will be $\frac{1}{n^k}$ in any dimension.

From our estimation, the growth function should be proportional to $\frac{n}{n^{1/k}}$, which indeed it is.  We found from empirical evidence that the constant scaling factor required to fit our data the best was $\frac{2}{3}$.

As for the "0 dimensional case" (random weights), we would expect it to be roughly similar to the 1-dimensional case since all edges are chosen to have weights uniformly between 0 and 1.  Indeed, as our above formula predicted, the average weight of the MST in "0D" is constant with respect to $n$.  However, it seems to be bounded at 1.2 rather than $\frac{2}{3}$, which may be due to the fact that we are making a single draw from a uniform to determine the weight rather than taking the difference of two draws.

\end{problem}

\begin{problem}{}
Which algorithm did you use and why?

We chose to implement Kruskal's algorithm, which required us to implement a disjoint heap data structure and a sorting algorithm.  Our decision to use Kruskal's algorithm came out of a choice between the two most commonly used algorithms, which are both fairly feasible to implement - Kruskal's and Prim's.

Prim's algorithm has a better amortized runtime for dense graphs, although the amortized runtime of its best reported implementation ($O(E + VlogV)$) requires the programmer to implement a Fibonacci heap, which is a rather daunting task.  Kruskal's algorithm has the benefit of using simpler data structures, and in general graphs its $O(ElogV)$ runtime is not too shabby.  Additionally, the disjoint set data structure and the sort that we had to implement would be good targets for optimization.  For the disjoint set, we implemented it as a tree (as discussed in class) utilizing path compression for the traversal operations to amortize future lookups.  The sort we implemented was a merge sort, which runs in a solid $O(nlog(n))$.

While we knew we would be testing complete graphs, we also knew that due to the uniform random distribution of the points/weights, we would be able to throw out a significant number of edges before running Kruskal's algorithm.  Within the same dimension, as the number of vertices increased, we noticed the maximum weight of an edge included in the spanning tree decreased in a predictable fashion.  This was because as more nodes are added to a complete graph with uniformly random weights (or weights derived from uniformly random points), there is a higher chance of finding a shorter path that spans a given node.  By running multiple iterations of our algorithm on graphs of varying size and dimension, we were able to fit a size-dependent threshold function for each dimension that would give a ceiling for the weights of edges that could be included in the minimum spanning tree (shown below).

As it turned out, the maximum weight of any edge added to a minimum spanning tree was well bounded by a function proportional to $\frac{d}{log(n)}$, where $n$ is the number of nodes in the function and $d$ was the dimension of the graph. We used $\frac{1}{log(n)}$ for the random weights variant ($d=0$) and $\frac{d}{log(n)}$ for dimension 2-4 as a threshold beyond which edges were discarded.

Thus, as our program generates graphs, it automatically discards edges with weights above the value of the threshold function.  This significantly reduced the size of the problem, improving both the space and runtime efficiency of Kruskal's algorithm.

\end{problem}

\begin{problem}{}
Are the growth rates surprising? Can you come up with an explanation
for them?

The growth rates are not entirely surprising.  As expected, the average tree size increased with respect to the number of nodes since there are more nodes to cover, making the MST larger.  Slightly less intuitive is the growth with respect to the dimension.  For a given number of points, the higher dimension graphs have a heavier average MST.  This is, however, consistent with expectations after considering a little math:

Since each coordinate of each point is drawn uniformly at random, let $x$ be the expected difference in any coordinate between any pair of points.  Since the distance between two points is the square root of the sum of the squared differences, we'd expect the distance between two points to be roughly proportional to $\sqrt{2n^2}$ in dimension 2, or generally $\sqrt{kn^2}$ in dimension $k$.  This is not as sound a foundation as described above in our explanation of the function bounding the growth rates, but gives some intuition behind the growth.

\end{problem}

\begin{problem}{}
How long does it take your alg to run? Does this make sense? Do you
notice things like the cache size of your computer having an effect?

\end{problem}

\begin{problem}{}
Did you have any interesting experiences with the random number
generator? Do you trust it?

When we first wrote our code, we seeded the random number generator with the C system time.  This worked fine for single runs on a graph of a given size (or multiple runs on graphs of large size), but ended up producing the same "random" values for repeated iterations of small graph sizes.  This was because the C system time is reported in seconds, and once we realized this we switched over to the posix "gettimeofday()" function, which allowed us to seed our random number generator with system time in microseconds times system time in seconds.  While this works for our purposes (we empirically tested on small cases to make sure this was generating a good degree of randomness), it is less than ideal to have a ceiling on how fast our program can run and still produce "random" output.  

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}